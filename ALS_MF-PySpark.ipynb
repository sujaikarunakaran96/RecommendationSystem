{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Dkq72ecC37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTebIGVacccA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tvTywX2chOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tGmU63fcm_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.sql import Row"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrMlo1AKe2HG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inV8Ze1Ofhx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYGlP_w8em2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/Reviews copy.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmLFV2i6wh4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ratings = spark.createDataFrame(df_r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUzf65iFkLkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(ratings) for column in list(set(ratings.columns)-set(['date'])) ]\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "df_r = pipeline.fit(ratings).transform(ratings)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ucT-xjuoR8",
        "colab_type": "code",
        "outputId": "5ef16f93-bfa6-4ed0-ce5f-85042da5585e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df_r.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Score', 'bigint'),\n",
              " ('ProductId_index', 'double'),\n",
              " ('UserId_index', 'double')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEpUR5OnvVc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns_to_drop = [\"Id_index\", \"ProductId\", \"User_Id\", \"Id\", \"Score_index\"]\n",
        "\n",
        "df_r = df_r.drop(*columns_to_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuZHjdAvqL8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_r.drop(\"Id\", \"ProductId\", \"UserId\", \"Score_index\").collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IELaasRsg6l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the recommendation model using Alternating Least Squares\n",
        "# split training and testing\n",
        "(training, test) = df_r.randomSplit([0.8, 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwuchAcChDt0",
        "colab_type": "code",
        "outputId": "416d91da-4899-4fee-827e-fec96ed6ec19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Building the recommendation model by ALS\n",
        "als_old = ALS(userCol=\"UserId_index\", itemCol=\"ProductId_index\", ratingCol=\"Score\", nonnegative=True, coldStartStrategy= \"drop\")\n",
        "model_old = als_old.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions_old = model_old.transform(test)\n",
        "evaluator_old = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Score\",\n",
        "                                predictionCol=\"prediction\")\n",
        "\n",
        "rmse_old = evaluator_old.evaluate(predictions_old)\n",
        "print(\"Root-mean-square error = \" + str(rmse_old))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root-mean-square error = 1.2046699187526728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAuXllezzZBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Improve the performance of the model by using CROSS VALIDATION and PARAMETER TUNING\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "\n",
        "\n",
        "#create a new ALS estimator\n",
        "als = ALS(userCol=\"UserId_index\", itemCol=\"ProductId_index\", ratingCol=\"Score\", coldStartStrategy=\"drop\")\n",
        "#define a grid for both parameters\n",
        "#this will test 9 different combinations of the 2 parameters\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [1, 5, 10, 25, 50]) \\\n",
        "    .addGrid(als.regParam, [.001,0.1, .01, 1, 10]) \\\n",
        "    .build()\n",
        "\n",
        "\n",
        "\n",
        "trainValSplit = TrainValidationSplit(estimator = als, estimatorParamMaps=paramGrid, \n",
        "                                    evaluator = RegressionEvaluator(metricName=\"rmse\", predictionCol=\"prediction\", labelCol=\"Score\"), \n",
        "                                     trainRatio = 0.9, parallelism = 2)\n",
        "\n",
        "\n",
        "\n",
        "model = trainValSplit.fit(training)\n",
        "\n",
        "\n",
        "# Finally retrieve the best model\n",
        "bestModel = model.bestModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE6hBXORIzOB",
        "colab_type": "code",
        "outputId": "d7536315-58d7-4279-9075-7c23f1cc40f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Generate top 10 movie recommendations for each user\n",
        "userRecs = model_old.recommendForAllUsers(10)\n",
        "userRecs.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "219736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Q-_ALpoiw1N",
        "colab_type": "code",
        "outputId": "d0eaf784-1a40-42d9-e614-fa146bcde7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "userRecs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[UserId_index: int, recommendations: array<struct<ProductId_index:int,rating:float>>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaSSZfZWfpYz",
        "colab_type": "code",
        "outputId": "ebccb823-8285-4e6c-aa61-89d03a52e4c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "userRecs.iloc[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-a73c99cc60b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muserRecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1304\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1305\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iloc'"
          ]
        }
      ]
    }
  ]
}